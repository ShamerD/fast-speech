{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookPath":"main.ipynb","language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}},"notebookId":"3ae197e6-a705-46a1-a7ca-deb06969c466","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{"cellId":"guwye8ke0vhvn320lx43z"}},{"cell_type":"code","source":"#!g1.1\n!git clone https://github.com/ShamerD/fast-speech.git","metadata":{"cellId":"gp66xu8mndwkrxtmv09j6h","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Cloning into 'fast-speech'...\nremote: Enumerating objects: 201, done.\u001B[K\nremote: Counting objects: 100% (201/201), done.\u001B[K\nremote: Compressing objects: 100% (145/145), done.\u001B[K\nremote: Total 201 (delta 90), reused 164 (delta 53), pack-reused 0\u001B[K\nReceiving objects: 100% (201/201), 37.87 KiB | 945.00 KiB/s, done.\nResolving deltas: 100% (90/90), done.\n"}],"execution_count":2},{"cell_type":"code","source":"#!g1.1\n%pwd","metadata":{"cellId":"x6l9uz4cegaobljql96qg","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"'/home/jupyter/work/resources/fast-speech'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"#!g1.1\n%cd fast-speech\n%pwd","metadata":{"cellId":"9zon5isqdfrxk0yz6rs7m","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"/home/jupyter/work/resources/fast-speech\n"},{"output_type":"display_data","data":{"text/plain":"'/home/jupyter/work/resources/fast-speech'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"#!g1.1\n!git clone https://github.com/NVIDIA/waveglow.git\n%pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"jupyter":{"outputs_hidden":true},"cellId":"rmvn469k65yqwtnmvfj2","trusted":true,"collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Cloning into 'waveglow'...\nremote: Enumerating objects: 190, done.\u001B[K\nremote: Total 190 (delta 0), reused 0 (delta 0), pack-reused 190\u001B[K\nReceiving objects: 100% (190/190), 435.59 KiB | 709.00 KiB/s, done.\nResolving deltas: 100% (106/106), done.\nDefaulting to user installation because normal site-packages is not writeable\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==1.10.0+cu111\n  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB)\n\u001B[K     |████████████████████████████████| 2137.6 MB 95 bytes/s \n\u001B[?25hCollecting torchaudio==0.10.0+cu111\n  Downloading https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2.9 MB)\n\u001B[K     |████████████████████████████████| 2.9 MB 76.7 MB/s \n\u001B[?25hCollecting librosa==0.8.1\n  Downloading librosa-0.8.1-py3-none-any.whl (203 kB)\n\u001B[K     |████████████████████████████████| 203 kB 2.2 MB/s \n\u001B[?25hCollecting pandas==1.3.4\n  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n\u001B[K     |████████████████████████████████| 11.3 MB 10.2 MB/s \n\u001B[?25hCollecting numpy==1.20.3\n  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n\u001B[K     |████████████████████████████████| 15.3 MB 1.7 MB/s \n\u001B[?25hCollecting scipy==1.7.3\n  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n\u001B[K     |████████████████████████████████| 38.1 MB 129 kB/s \n\u001B[?25hRequirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.4)\nCollecting matplotlib==3.5.0\n  Downloading matplotlib-3.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n\u001B[K     |████████████████████████████████| 11.2 MB 67.7 MB/s \n\u001B[?25hCollecting wandb==0.12.7\n  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n\u001B[K     |████████████████████████████████| 1.7 MB 52.1 MB/s \n\u001B[?25hCollecting tqdm==4.62.3\n  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n\u001B[K     |████████████████████████████████| 76 kB 6.7 MB/s \n\u001B[?25hRequirement already satisfied: Pillow==8.4.0 in /kernel/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (8.4.0)\nRequirement already satisfied: decorator>=3.0.0 in /kernel/lib/python3.7/site-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (5.1.0)\nRequirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (0.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (1.0.1)\nRequirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (1.4.0)\nRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (2.1.9)\nRequirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (0.10.3.post1)\nRequirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (0.51.2)\nRequirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (0.22.1)\nRequirement already satisfied: packaging>=20.0 in /kernel/lib/python3.7/site-packages (from librosa==0.8.1->-r requirements.txt (line 4)) (20.9)\nCollecting setuptools-scm>=4\n  Downloading setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\nRequirement already satisfied: pyparsing>=2.2.1 in /kernel/lib/python3.7/site-packages (from matplotlib==3.5.0->-r requirements.txt (line 11)) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /kernel/lib/python3.7/site-packages (from matplotlib==3.5.0->-r requirements.txt (line 11)) (0.11.0)\nCollecting fonttools>=4.22.0\n  Downloading fonttools-4.28.2-py3-none-any.whl (880 kB)\n\u001B[K     |████████████████████████████████| 880 kB 54.8 MB/s \n\u001B[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /kernel/lib/python3.7/site-packages (from matplotlib==3.5.0->-r requirements.txt (line 11)) (1.3.2)\nRequirement already satisfied: python-dateutil>=2.7 in /kernel/lib/python3.7/site-packages (from matplotlib==3.5.0->-r requirements.txt (line 11)) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->-r requirements.txt (line 6)) (2021.1)\nRequirement already satisfied: typing-extensions in /kernel/lib/python3.7/site-packages (from torch==1.10.0+cu111->-r requirements.txt (line 1)) (4.0.0)\nRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (2.3)\nCollecting docker-pycreds>=0.4.0\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (7.1.2)\nRequirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (4.0.2)\nRequirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (1.0.1)\nRequirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (3.1.18)\nRequirement already satisfied: six>=1.13.0 in /kernel/lib/python3.7/site-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (1.16.0)\nRequirement already satisfied: psutil>=5.0.0 in /kernel/lib/python3.7/site-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (5.7.3)\nCollecting sentry-sdk>=1.0.0\n  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n\u001B[K     |████████████████████████████████| 140 kB 91.2 MB/s \n\u001B[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (3.17.3)\nCollecting pathtools\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (5.3.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /kernel/lib/python3.7/site-packages (from wandb==0.12.7->-r requirements.txt (line 12)) (2.25.1)\nCollecting subprocess32>=3.5.3\n  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n\u001B[K     |████████████████████████████████| 97 kB 10.6 MB/s \n\u001B[?25hCollecting yaspin>=1.0.0\n  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb==0.12.7->-r requirements.txt (line 12)) (4.0.7)\nRequirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.12.7->-r requirements.txt (line 12)) (4.0.0)\nRequirement already satisfied: setuptools in /kernel/lib/python3.7/site-packages (from numba>=0.43.0->librosa==0.8.1->-r requirements.txt (line 4)) (51.0.0)\nRequirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.1->-r requirements.txt (line 4)) (0.34.0)\nRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.1->-r requirements.txt (line 4)) (1.4.4)\nRequirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb==0.12.7->-r requirements.txt (line 12)) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb==0.12.7->-r requirements.txt (line 12)) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb==0.12.7->-r requirements.txt (line 12)) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb==0.12.7->-r requirements.txt (line 12)) (4.0.0)\nCollecting tomli>=1.0.0\n  Downloading tomli-1.2.2-py3-none-any.whl (12 kB)\nRequirement already satisfied: cffi>=1.0 in /kernel/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa==0.8.1->-r requirements.txt (line 4)) (1.15.0)\nRequirement already satisfied: pycparser in /kernel/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.8.1->-r requirements.txt (line 4)) (2.21)\nRequirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb==0.12.7->-r requirements.txt (line 12)) (1.1.0)\nBuilding wheels for collected packages: subprocess32, pathtools\n  Building wheel for subprocess32 (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6488 sha256=75fd63187a6751bccbf61865d7d0828ff839e41cad34c1246ea89f35a07e0643\n  Stored in directory: /tmp/xdg_cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n  Building wheel for pathtools (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=22457e3e76d6ff6d5ba89b3b3667a02abae30f338bf020775b2d11458d356db3\n  Stored in directory: /tmp/xdg_cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\nSuccessfully built subprocess32 pathtools\nInstalling collected packages: numpy, tomli, scipy, yaspin, torch, subprocess32, setuptools-scm, sentry-sdk, pathtools, fonttools, docker-pycreds, wandb, tqdm, torchaudio, pandas, matplotlib, librosa\n\u001B[33m  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\n\u001B[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/jupyter/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\n\u001B[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/jupyter/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\n\u001B[33m  WARNING: The scripts wandb and wb are installed in '/home/jupyter/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\n\u001B[33m  WARNING: The script tqdm is installed in '/home/jupyter/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nml-kernel 0.0.1 requires matplotlib<=3.3.3, but you have matplotlib 3.5.0 which is incompatible.\ntorchvision 0.7.0 requires torch==1.6.0, but you have torch 1.10.0+cu111 which is incompatible.\nthinc 8.0.8 requires typing-extensions<4.0.0.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.0.0 which is incompatible.\nspacy 3.1.1 requires typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.0.0 which is incompatible.\nmmdet 2.3.0rc0+c6b5ca2 requires Pillow<=6.2.2, but you have pillow 8.4.0 which is incompatible.\nmmdet 2.3.0rc0+c6b5ca2 requires torch==1.6.0, but you have torch 1.10.0+cu111 which is incompatible.\nkaggle 1.5.8 requires urllib3<1.25,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\nenot-utils 1.0.2 requires torch==1.6.0, but you have torch 1.10.0+cu111 which is incompatible.\ncloud-ml 0.0.1 requires tqdm<=4.54.1,>=4.45.0, but you have tqdm 4.62.3 which is incompatible.\u001B[0m\nSuccessfully installed docker-pycreds-0.4.0 fonttools-4.28.2 librosa-0.8.1 matplotlib-3.5.0 numpy-1.20.3 pandas-1.3.4 pathtools-0.1.2 scipy-1.7.3 sentry-sdk-1.5.0 setuptools-scm-6.3.2 subprocess32-3.5.4 tomli-1.2.2 torch-1.10.0+cu111 torchaudio-0.10.0+cu111 tqdm-4.62.3 wandb-0.12.7 yaspin-2.1.0\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n"}],"execution_count":6},{"cell_type":"code","source":"#!g1.1\nimport wandb\nwandb.login()","metadata":{"cellId":"dq7znzxu3qmu6eome4knx","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"},{"output_type":"stream","name":"stdin","text":"\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"},{"output_type":"stream","name":"stderr","text":"\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"},{"output_type":"display_data","data":{"text/plain":"True"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"#!g1.1\n!git pull","metadata":{"cellId":"ng7vu7tubzs1hx3xe4ecb","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"remote: Enumerating objects: 9, done.\u001B[K\nremote: Counting objects: 100% (9/9), done.\u001B[K\nremote: Compressing objects: 100% (2/2), done.\u001B[K\nremote: Total 5 (delta 3), reused 5 (delta 3), pack-reused 0\u001B[K\nUnpacking objects: 100% (5/5), done.\nFrom https://github.com/ShamerD/fast-speech\n   0ca3968..ad9881c  main       -> origin/main\nUpdating 0ca3968..ad9881c\nFast-forward\n src/loss/FastSpeechLoss.py | 4 \u001B[32m++\u001B[m\u001B[31m--\u001B[m\n 1 file changed, 2 insertions(+), 2 deletions(-)\n"}],"execution_count":30},{"cell_type":"code","source":"#!g1.1\n!python3 train.py -c src/config.json","metadata":{"cellId":"imtq2m5r6we48kgwkclfut","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"FastSpeech(\n  (encoder): Encoder(\n    (emb): Embedding(38, 384)\n    (pos_emb): Embedding(1024, 384)\n    (net): Sequential(\n      (0): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (1): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (2): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (3): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (4): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (5): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (length_regulator): LengthRegulator(\n    (conv1): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n    (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (drop1): Dropout(p=0.1, inplace=False)\n    (drop2): Dropout(p=0.1, inplace=False)\n    (predictor): Linear(in_features=256, out_features=1, bias=True)\n  )\n  (decoder): Decoder(\n    (pos_emb): Embedding(2048, 384)\n    (net): Sequential(\n      (0): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (1): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (2): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (3): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (4): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n      (5): FFTBlock(\n        (mha): MultiHeadSelfAttention(\n          (Q): Linear(in_features=384, out_features=384, bias=True)\n          (K): Linear(in_features=384, out_features=384, bias=True)\n          (V): Linear(in_features=384, out_features=384, bias=True)\n          (out): Linear(in_features=384, out_features=384, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (conv): Sequential(\n          (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n          (1): ReLU()\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        )\n        (ln_mha): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln_conv): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (drop_mha): Dropout(p=0.1, inplace=False)\n        (drop_conv): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (mel_pred): Linear(in_features=384, out_features=80, bias=True)\n)\n\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mshamerd\u001B[0m (use `wandb login --relogin` to force relogin)\n\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.12.7\n\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33mvolcanic-violet-10\u001B[0m\n\u001B[34m\u001B[1mwandb\u001B[0m: ⭐️ View project at \u001B[34m\u001B[4mhttps://wandb.ai/shamerd/tts_project\u001B[0m\n\u001B[34m\u001B[1mwandb\u001B[0m: 🚀 View run at \u001B[34m\u001B[4mhttps://wandb.ai/shamerd/tts_project/runs/2vo1hqk6\u001B[0m\n\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in /home/jupyter/work/resources/fast-speech/wandb/run-20211201_133804-2vo1hqk6\n\u001B[34m\u001B[1mwandb\u001B[0m: Run `wandb offline` to turn off syncing.\n\ntrain:   0%|                                           | 0/1000 [00:00<?, ?it/s]\ntrain:  10%|███▎                             | 100/1000 [02:16<20:09,  1.34s/it]\ntrain:  20%|██████▌                          | 200/1000 [04:33<17:57,  1.35s/it]\ntrain:  30%|█████████▉                       | 300/1000 [06:51<15:40,  1.34s/it]\ntrain:  40%|█████████████▏                   | 400/1000 [09:08<13:26,  1.34s/it]\ntrain:  50%|████████████████▌                | 500/1000 [11:25<11:14,  1.35s/it]\ntrain:  60%|███████████████████▊             | 600/1000 [13:42<08:57,  1.34s/it]\ntrain:  70%|███████████████████████          | 700/1000 [15:59<06:46,  1.35s/it]\ntrain:  80%|██████████████████████████▍      | 800/1000 [18:17<04:31,  1.36s/it]\ntrain:  90%|█████████████████████████████▋   | 900/1000 [20:35<02:15,  1.35s/it]\ntrain: 100%|████████████████████████████████▉| 999/1000 [22:51<00:01,  1.35s/it]\n\n\n\n\n\n\ntrain:   0%|                                           | 0/1000 [00:00<?, ?it/s]\n\n\ntrain:  10%|███▎                             | 100/1000 [02:17<20:16,  1.35s/it]\ntrain:  20%|██████▌                          | 200/1000 [04:35<18:10,  1.36s/it]\ntrain:  30%|█████████▉                       | 300/1000 [06:53<15:50,  1.36s/it]\ntrain:  40%|█████████████▏                   | 400/1000 [09:11<13:32,  1.35s/it]\ntrain:  50%|████████████████▌                | 500/1000 [11:29<11:16,  1.35s/it]\ntrain:  60%|███████████████████▊             | 600/1000 [13:47<09:02,  1.36s/it]\ntrain:  70%|███████████████████████          | 700/1000 [16:05<06:45,  1.35s/it]\ntrain:  80%|██████████████████████████▍      | 800/1000 [18:23<04:31,  1.36s/it]\ntrain:  90%|█████████████████████████████▋   | 900/1000 [20:41<02:21,  1.42s/it]\ntrain: 100%|████████████████████████████████▉| 999/1000 [22:57<00:01,  1.36s/it]\n\n\n\n\n\ntrain:   0%|                                           | 0/1000 [00:00<?, ?it/s]\ntrain:  10%|███▎                             | 100/1000 [02:17<20:13,  1.35s/it]\ntrain:  20%|██████▌                          | 200/1000 [04:36<18:04,  1.36s/it]\ntrain:  30%|█████████▉                       | 300/1000 [06:54<15:50,  1.36s/it]\ntrain:  40%|█████████████▏                   | 400/1000 [09:12<13:32,  1.35s/it]\ntrain:  50%|████████████████▌                | 500/1000 [11:31<11:21,  1.36s/it]\ntrain:  60%|███████████████████▊             | 600/1000 [13:49<09:05,  1.36s/it]\ntrain:  70%|███████████████████████          | 700/1000 [16:07<06:47,  1.36s/it]\ntrain:  80%|██████████████████████████▍      | 800/1000 [18:26<04:31,  1.36s/it]\ntrain:  90%|█████████████████████████████▋   | 900/1000 [20:45<02:16,  1.36s/it]\ntrain: 100%|████████████████████████████████▉| 999/1000 [23:02<00:01,  1.37s/it]\n\n\n\n\n\ntrain:   0%|                                           | 0/1000 [00:00<?, ?it/s]\ntrain:  10%|███▎                             | 100/1000 [02:18<20:23,  1.36s/it]\ntrain:  20%|██████▌                          | 200/1000 [04:37<18:10,  1.36s/it]\ntrain:  23%|███████▍                         | 227/1000 [05:16<17:28,  1.36s/it]^C\n\n\n"},{"output_type":"error","ename":"Exception","evalue":"Process exited with code -1","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)","\u001B[0;32m<ipython-input-16-52ec8ec17ea5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'python3 train.py -c src/config.json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(code)\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    262\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_script_executor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mScriptExecutor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mProcessHandler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msystem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 263\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshell\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msystem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mcode\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_script_executor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"bash\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_init_message_handlers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/kernel/lib/python3.7/site-packages/ml_kernel/script_executor.py\u001B[0m in \u001B[0;36mexecute\u001B[0;34m(self, lang, code)\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0mreturn_code\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_system\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mreturn_code\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Process exited with code %d'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mreturn_code\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;31mException\u001B[0m: Process exited with code -1"]}],"execution_count":31},{"cell_type":"code","source":"#!g1.1\nwandb.finish(1)","metadata":{"cellId":"js7uofc3q7pz91r3wpijb","trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"sawcqvy77kegr6jdyfcpuw"},"outputs":[],"execution_count":null}]}